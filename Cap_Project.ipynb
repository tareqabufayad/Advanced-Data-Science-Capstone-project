{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Word Clustering from Large Arabic Text\n",
    "In this Jupyter Notebook, I show a solution of creating semantic word clustering from large Arabic plain unstructured text. This approach uses neural word embedding models “word2vec library” to create word vectors and avoid using one-hot encoding which is not consider the semantic relationships between Arabic words. Also, the beauty of embedding approach is that not much feature engineering is needed. This approach is part of my thesis work for my master degree of information technology in 2018. I will use word2vec google library to create word vectors and then create classification features and finally creates word clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and Preprocessing Data\n",
    "I download the Wikimedia database dump of Arabic Wikipedia on May 20, 2017 (https://archive.org/details/arwiki-20170520) . The volume of Arabic Wikimedia articles has reached as of September 28, 2017 above 510,651 articles (from different domains such as politics, economy, comedy, history and others). The text volume about (1.7GB) and become (1.3GB) after pre-processing. The preprocessing stage consists of dropping the diacritical marks and the character elongation and data normalization (dropping any character and symbols except Arabic characters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import codecs\n",
    "import pyarabic.araby as araby\n",
    "import timeit\n",
    "\n",
    "\n",
    "def read_file():\n",
    "    f = codecs.open(\"3g.txt\", \"r\", encoding='utf8')\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_file(data):\n",
    "    filew = codecs.open('3g-p.txt', 'w', encoding='utf8')\n",
    "    filew.write(data)\n",
    "\n",
    "    filew.close()\n",
    "\n",
    "\n",
    "def write_one_col():\n",
    "    with open('10-wiki.txt', 'r') as f, open('one-col.txt', 'w') as f2:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                f2.write(word + '\\n')\n",
    "\n",
    "\n",
    "def normailize_data(data):\n",
    "    regex = ur'[\\u0621-\\u063A\\u0641-\\u064A]+'\n",
    "    return \" \".join(re.findall(regex, data))\n",
    "\n",
    "\n",
    "def strip_tatweel(text):\n",
    "\n",
    "    reduced = araby.strip_tatweel(text)\n",
    "    return reduced\n",
    "\n",
    "\n",
    "def strip_tashkeel(text):\n",
    "    reduced = araby.strip_tashkeel(text)\n",
    "    return reduced\n",
    "\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "data = read_file()\n",
    "remove_tashkeel = strip_tashkeel(data)\n",
    "remove_tatweel = strip_tatweel(remove_tashkeel)\n",
    "normailized = normailize_data(remove_tatweel)\n",
    "write_file(normailized)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "InMinutes = elapsed / 60\n",
    "\n",
    "print (\"The Totatl Execution Time in Minutes is: \", InMinutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Word Vectors\n",
    "We create word vectors using word2vec model. Two stages are applied to create word vectors. First; word phrases are created to constitute the bigram statistics for the word frequency in text corpus, then they are used as better input for word2vec model. Second; creating the vector representation of words using the CBOW model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import word2vec\n",
    "import timeit\n",
    "\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "word2vec.word2phrase('3g-p.txt', '3g-phrases.txt', verbose=True)\n",
    "word2vec.word2vec('3g-phrases.txt', '3g.bin', size=100, verbose=True)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "InMinutes = elapsed / 60\n",
    "\n",
    "word2vec.word2clusters('3g-p.txt', '3g-clusters.txt', 100, verbose=True)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('3g.bin', binary=True)\n",
    "\n",
    "model.save_word2vec_format('3g-vectors.txt', binary=False)\n",
    "\n",
    "print (\"The Totatl Execution Time in Minutes is: \", InMinutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "Building the classification features for the classification tasks is performed by constructing Term Frequency Inverse Document Matrix (TF-IDF). This matrix scores importance of words or terms in a document based on how frequently they appear across multiple documents. In order to construct TF-IDF matrix from the generated word vectors, we average the word vectors for each word. We used a GitHub repository class (MeanEmbeddingVectorizer) written by Nadbordrozd (Nadbordrozd, 2016)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import data\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "#Store the word vectors into dictionary\n",
    "with open(\"1900m-vectors.txt\", \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}\n",
    "\n",
    "# build the features, by averaging the word vectors for all vectors in the text\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification models\n",
    "In order to classify the generated word vectors and create the classification model, we used the Sklearn’s pipeline. The pipeline sequentially applies a list of transforms (such as extracting text documents and tokenizes them) before passing the resulting features along to classifier algorithms. We used the pipeline object to transform the process of averaging word vectors and creating classification features and passes these features to extra tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the vectors using Extra Trees classifier\n",
    "model = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "xx = data.get_data('train.txt')\n",
    "yy = data.label_data('label.txt')\n",
    "\n",
    "X = np.array(xx)\n",
    "y = np.array([yy]).reshape(150)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=None)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    data_train, data_test = X[train_index], X[test_index]\n",
    "    target_train, target_test = y[train_index], y[test_index]\n",
    "\n",
    "model.fit(data_train, target_train)\n",
    "\n",
    "prediction = model.predict(data_test)\n",
    "print(pd.DataFrame({'words': data_test, 'prediction': prediction}))\n",
    "\n",
    "expected = target_test\n",
    "predicted = model.predict(data_test)\n",
    "\n",
    "cm = confusion_matrix(target_test, prediction)\n",
    "\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print accuracy_score(target_test, predicted)\n",
    "\n",
    "print  model_selection.cross_val_score(model, data_train, target_train)\n",
    "\n",
    "print('the classification Scores:\\n')\n",
    "\n",
    "target_names = ['class 1', 'class 2', 'class 3', 'class 4', 'class 5',\n",
    "                'class 6', 'class 7', 'class 8', 'class 9', 'class 10',\n",
    "                'class 11', 'class 12', 'class 13', 'class 14', 'class 15']\n",
    "\n",
    "print '\\nClasification report:\\n', classification_report(target_test, prediction, target_names=target_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
